{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XZp4Be3z4eJA"
   },
   "source": [
    "<hr style=\"height:5px; background-color:black; border:none;\">\n",
    "\n",
    "## **Assignment Name:** Week 1 & 2 Coding Assignment\n",
    "\n",
    "### **Course Name:** DSC530\n",
    "\n",
    "### **Student Name:** Tim Hollis\n",
    "\n",
    "### **Date:** 12/07/2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment Instructions:\n",
    "\n",
    "**Exercise 1**\n",
    "\n",
    "We want to look at data for the Facebook, Apple, Amazon, Netflix, and Google (FAANG) stocks, but we were given each as a separate CSV file (obtained using the stock_analysis package we will build in Chapter 7, Financial Analysis – Bitcoin and the Stock Market). Combine them into a single file and store the dataframe of the FAANG data as faang for the rest of the exercises:\n",
    "Read in the aapl.csv, amzn.csv, fb.csv, goog.csv, and nflx.csv files.\n",
    "Add a column to each dataframe, called ticker, indicating the ticker symbol it is for (Apple's is AAPL, for example); this is how you look up a stock. In this case, the filenames happen to be the ticker symbols.\n",
    "Append them together into a single dataframe.\n",
    "Save the result in a CSV file called faang.csv\n",
    "\n",
    "**Exercise 2**\n",
    "\n",
    "With faang, use type conversion to cast the values of the date column into datetimes and the volume column into integers. Then, sort by date and ticker.\n",
    "\n",
    "**Exercise 3**\n",
    "\n",
    "Find the seven rows in faang with the lowest value for volume.\n",
    "\n",
    "**Exercise 4**\n",
    "\n",
    "Right now, the data is somewhere between long and wide format. Use melt() to make it completely long format. Hint: date and ticker are our ID variables (they uniquely identify each row). We need to melt the rest so that we don't have separate columns for open, high, low, close, and volume.\n",
    "\n",
    "**Exercise 5**\n",
    "\n",
    "Suppose we found out that on July 26, 2018 there was a glitch in how the data was recorded. How should we handle this? Note that there is no coding required for this exercise.\n",
    "\n",
    "**Exercise 6**\n",
    "\n",
    "The European Centre for Disease Prevention and Control (ECDC) provides an open dataset on COVID-19 cases called daily number of new reported cases of COVID-19 by country worldwide. This dataset is updated daily, but we will use a snapshot that contains data from January 1, 2020 through September 18, 2020. Clean and pivot the data so that it is in wide format:\n",
    "Read in the covid19_cases.csv file.\n",
    "Create a date column using the data in the dateRep column and the pd.to_datetime() function.\n",
    "Set the date column as the index and sort the index.\n",
    "Replace all occurrences of United_States_of_America and United_Kingdom with USA and UK, respectively. Hint: the replace() method can be run on the dataframe as a whole.\n",
    "Using the countriesAndTerritories column, filter the cleaned COVID-19 cases data down to Argentina, Brazil, China, Colombia, India, Italy, Mexico, Peru, Russia, Spain, Turkey, the UK, and the USA.\n",
    "Pivot the data so that the index contains the dates, the columns contain the country names, and the values are the case counts (the cases column). Be sure to fill in NaN values with 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:3px; background-color:black; border:none;\">\n",
    "\n",
    "### Import Libraries and Load Data\n",
    "\n",
    "<hr style=\"border-top:3px dotted red;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: C:\\Users\\slimt\\DSC530\\Hands-On-Data-Analysis-with-Pandas-2nd-edition-master\\ch_03\\exercises\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Path relevant to current working directory\n",
    "base_path = Path.cwd() / 'Hands-On-Data-Analysis-with-Pandas-2nd-edition-master' / \\\n",
    "    'ch_03' / 'exercises'\n",
    "\n",
    "print(f\"Working directory: {base_path}\")\n",
    "\n",
    "# Display settings for better output\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.float_format', lambda x: f'{x:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<hr style=\"border-top:3px dotted red;\">\n",
    "\n",
    "## **Data Manipulation with Pandas Assignment Setup**\n",
    "\n",
    "This assignment uses two datasets provided with Chapter 3 of *Hands-On Data Analysis with Pandas*:\n",
    "\n",
    "1. **FAANG Stock Data** (2014-2018)\n",
    "\n",
    "   - Apple (AAPL), Amazon (AMZN), Facebook (FB), Google (GOOG), Netflix (NFLX)\n",
    "   - Generated using the textbook's `stock_analysis` package\n",
    "   - Files: `aapl.csv`, `amzn.csv`, `fb.csv`, `goog.csv`, `nflx.csv`\n",
    "\n",
    "2. **COVID-19 Case Data** (Jan 1 - Sep 18, 2020)\n",
    "\n",
    "   - Source: European Centre for Disease Prevention and Control (ECDC)\n",
    "   - Daily new reported cases by country worldwide\n",
    "   - File: `covid19_cases.csv`\n",
    "\n",
    "<hr style=\"border-top:3px dotted red;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W58yhTdztMCt"
   },
   "source": [
    "<hr style=\"height:2px; background-color:black; border:none;\">\n",
    "\n",
    "## **Ch3 Ex1: Combining FAANG Stock Data**\n",
    "\n",
    "### **Objective:** \n",
    "\n",
    "Combine separate CSV files for FAANG stocks into a single dataframe.\n",
    "\n",
    "### **Steps to complete:**\n",
    "\n",
    "1. Read in the five CSV files (aapl.csv, amzn.csv, fb.csv, goog.csv, nflx.csv)\n",
    "2. Add a 'ticker' column to each dataframe with the appropriate stock symbol\n",
    "3. Append all dataframes together into one\n",
    "4. Save the combined result to 'faang.csv'\n",
    "\n",
    "<hr style=\"border-top:2px dashed blue;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "bv2g0VJVaBw2"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\slimt\\\\DSC530\\\\Hands-On-Data-Analysis-with-Pandas-2nd-edition-master\\\\ch_03\\\\exercises\\\\fb.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      8\u001b[39m tickers = [\u001b[33m'\u001b[39m\u001b[33mfb\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33maapl\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mamzn\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mnflx\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mgoog\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m ticker \u001b[38;5;129;01min\u001b[39;00m tickers:\n\u001b[32m     11\u001b[39m     \u001b[38;5;66;03m# Read each CSV file\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mticker\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m     \u001b[38;5;66;03m# Insert ticker column\u001b[39;00m\n\u001b[32m     15\u001b[39m     df.insert(\u001b[32m0\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mticker\u001b[39m\u001b[33m'\u001b[39m, ticker.upper())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\DSC530\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\DSC530\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\DSC530\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\DSC530\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\DSC530\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\slimt\\\\DSC530\\\\Hands-On-Data-Analysis-with-Pandas-2nd-edition-master\\\\ch_03\\\\exercises\\\\fb.csv'"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# Step 1: Initialize empty dataframe and read files in loop\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "# Start with empty dataframe\n",
    "faang = pd.DataFrame()\n",
    "\n",
    "tickers = ['fb', 'aapl', 'amzn', 'nflx', 'goog']\n",
    "\n",
    "for ticker in tickers:\n",
    "    # Read each CSV file\n",
    "    df = pd.read_csv(os.path.join(base_path, f'{ticker}.csv'))\n",
    "\n",
    "    # Insert ticker column\n",
    "    df.insert(0, 'ticker', ticker.upper())\n",
    "\n",
    "    # Append to the main dataframe\n",
    "    faang = pd.concat([faang, df], ignore_index=True)\n",
    "\n",
    "    print(f\"  ✓ Processed {ticker.upper()}: {len(df)} rows\")\n",
    "\n",
    "print(\"\\n✓ Step 1 Complete: All CSV files loaded and combined\")\n",
    "print(f\"  Total rows: {len(faang):,}\")\n",
    "print(f\"  Total columns: {len(faang.columns)}\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Step 2: Display sample data for verification\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "print('\\n✓ Step 2 Complete: Data combined')\n",
    "\n",
    "print('\\nFirst 5 rows of combined DataFrame:')\n",
    "display(faang.head())\n",
    "\n",
    "print('\\nLast 5 rows of combined DataFrame:')\n",
    "display(faang.tail())\n",
    "\n",
    "print('\\nCount by ticker (verify all data included):')\n",
    "ticker_counts = faang['ticker'].value_counts()\n",
    "for ticker, count in ticker_counts.items():\n",
    "    print(f\"  {ticker}: {count:,} rows\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Step 3: Save the combined result to 'faang.csv'\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "faang.to_csv(os.path.join(base_path, 'faang.csv'), index=False)\n",
    "\n",
    "# Verify the file was created\n",
    "if os.path.exists(os.path.join(base_path, 'faang.csv')):\n",
    "    file_size = os.path.getsize(os.path.join(base_path, 'faang.csv'))\n",
    "    print(f\"  File size: {file_size:,} bytes\")\n",
    "\n",
    "    print('\\n✓ Step 3 Complete: Saved to faang.csv')\n",
    "    print(f\"  File saved at: {os.path.join(base_path, 'faang.csv')}\")\n",
    "\n",
    "    # Load it back to verify structure\n",
    "    faang_loaded = pd.read_csv(os.path.join(base_path, 'faang.csv'))\n",
    "    print(f\"  Loaded verification: {faang.shape == faang_loaded.shape}\")\n",
    "    print(f\"  First column in saved file: '{faang_loaded.columns[0]}'\")\n",
    "    if faang_loaded.columns[0] == 'ticker':\n",
    "        print('  ✓ Structure Verified')\n",
    "else:\n",
    "    print('✗ Warning: File not found!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border-top:2px dashed blue;\">\n",
    "\n",
    "### **Exercise 1 Summary:**\n",
    "\n",
    "Successfully combined five separate stock CSV files (FB, AAPL, AMZN, NFLX, GOOG) into a single dataframe called `faang`, covering daily trading data for 2018. A new `ticker` column was added to preserve company identity, resulting in a unified dataset with 1,255 rows (251 trading days × 5 tickers) and 7 columns. Data integrity was verified by checking row counts per ticker, inspecting first and last records, and confirming the saved file structure (`faang.csv`). The consolidated dataset provides a clean foundation for cross‑company comparisons, time series analysis, and subsequent exercises in financial data exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tn0nOQb_9JDn"
   },
   "source": [
    "<hr style=\"height:2px; background-color:black; border:none;\">\n",
    "\n",
    "## **Ch3 Ex2: Data Type Conversion and Sorting**\n",
    "\n",
    "### **Objective:** \n",
    "\n",
    "Convert data types and sort the FAANG dataframe.\n",
    "\n",
    "### **Steps to complete:**\n",
    "\n",
    "1. Convert 'date' column to datetime format\n",
    "2. Convert 'volume' column to integer type\n",
    "3. Sort the dataframe by 'date' and 'ticker'\n",
    "\n",
    "<hr style=\"border-top:2px dashed blue;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vRVdK9dAaBw5"
   },
   "outputs": [],
   "source": [
    "# Note: Using the 'faang' DataFrame created in Exercise 1\n",
    "print(\"Using FAANG DataFrame from Exercise 1\")\n",
    "print(f\"DataFrame shape: {faang.shape}\")\n",
    "print(\"\\nCurrent data types:\")\n",
    "print(faang.dtypes)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Step 1: Convert 'date' column to datetime format\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "faang['date'] = pd.to_datetime(faang['date'])\n",
    "print(\"✓ 'date' column converted to datetime\")\n",
    "print(f\"  New data type: {faang['date'].dtype}\")\n",
    "print(f\"  Date range: {faang['date'].min()} to {faang['date'].max()}\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Step 2: Convert 'volume' column to integer type\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "faang['volume'] = faang['volume'].astype(int)\n",
    "print(\"✓ 'volume' column converted to integer\")\n",
    "print(f\"  New data type: {faang['volume'].dtype}\")\n",
    "print(\n",
    "    f\"  Volume range: {faang['volume'].min():,} \"\n",
    "    f\"to {faang['volume'].max():,}\"\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Step 3: Sort the dataframe by 'date' and 'ticker'\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "faang = faang.sort_values(['date', 'ticker']).reset_index(drop=True)\n",
    "print(\"✓ DataFrame sorted by date and ticker\")\n",
    "\n",
    "print(\"\\nFirst 10 rows after sorting:\")\n",
    "display(faang.head(10))\n",
    "print(\"\\nLast 5 rows after sorting:\")\n",
    "display(faang.tail())\n",
    "\n",
    "print(\"\\n✓ All conversions and sorting complete!\")\n",
    "print(\"\\nFinal data types:\")\n",
    "print(faang.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border-top:2px dashed blue;\">\n",
    "\n",
    "### **Exercise 2 Summary:**\n",
    "\n",
    "The FAANG dataset (1,255 rows × 7 columns) was refined for analysis by converting the `date` column to proper datetime format (covering Jan–Dec 2018) and the `volume` column to integer type, ensuring accurate numerical operations. The DataFrame was then sorted chronologically by date and alphabetically by ticker, producing a consistent structure for exploration. Validation checks confirmed the expected date range (2018‑01‑02 to 2018‑12‑31) and realistic trading volume values (679,000 to 384,986,800). These transformations prepare the dataset for reliable time series analysis, aggregation, and visualization in subsequent exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:3px; background-color:black; border:none;\">\n",
    "\n",
    "## **Ch3 Ex3: Finding Lowest Volume Rows**\n",
    "\n",
    "### **Objective:** \n",
    "\n",
    "Identify the seven rows with the lowest trading volume.\n",
    "\n",
    "### **Steps to complete:**\n",
    "\n",
    "1. Find the seven smallest values in the 'volume' column\n",
    "2. Extract and display these rows\n",
    "3. Analyze the results\n",
    "\n",
    "<hr style=\"border-top:2px dashed blue;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# Step 1: Find the seven rows with the lowest value for volume\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "lowest_volume = faang.nsmallest(7, 'volume')\n",
    "\n",
    "print(\"✓ Found 7 rows with lowest trading volume\")\n",
    "print(\"\\nSeven rows with lowest volume:\")\n",
    "display(lowest_volume)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Step 2: Display additional statistics for context\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "print(\"\\nVolume Statistics for Lowest Volume Rows:\")\n",
    "print(f\"  Minimum volume: {lowest_volume['volume'].min():,}\")\n",
    "print(f\"  Maximum volume (among these 7): {lowest_volume['volume'].max():,}\")\n",
    "print(f\"  Average volume: {lowest_volume['volume'].mean():,.0f}\")\n",
    "print(f\"  Median volume: {lowest_volume['volume'].median():,.0f}\")\n",
    "\n",
    "print(\"\\nAnalysis:\")\n",
    "print(f\"  Unique tickers affected: {lowest_volume['ticker'].nunique()}\")\n",
    "print(f\"  Tickers: {', '.join(lowest_volume['ticker'].unique())}\")\n",
    "\n",
    "print(f\"\\nDate range of low volume days:\")\n",
    "print(f\"  Earliest: {lowest_volume['date'].min().strftime('%Y-%m-%d')}\")\n",
    "print(f\"  Latest: {lowest_volume['date'].max().strftime('%Y-%m-%d')}\")\n",
    "days_span = (lowest_volume['date'].max() - lowest_volume['date'].min()).days\n",
    "print(f\"  Span: {days_span} days\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Show comparison to each stock's average volume\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CONTEXT: How low are these volumes compared to typical trading?\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for ticker in lowest_volume['ticker'].unique():\n",
    "    ticker_data = faang[faang['ticker'] == ticker]\n",
    "    ticker_avg = ticker_data['volume'].mean()\n",
    "    low_days = lowest_volume[lowest_volume['ticker'] == ticker]\n",
    "\n",
    "    for idx, row in low_days.iterrows():\n",
    "        date_str = row['date'].strftime('%Y-%m-%d')\n",
    "        volume = row['volume']\n",
    "        pct_of_avg = (volume / ticker_avg * 100)\n",
    "\n",
    "        print(f\"\\n{ticker} on {date_str}:\")\n",
    "        print(f\"  Volume: {volume:,}\")\n",
    "        print(f\"  {ticker} average volume: {ticker_avg:,.0f}\")\n",
    "        print(f\"  This is {pct_of_avg:.1f}% of {ticker}'s average volume\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Visualization: GOOG vs Other Stocks on Low Volume Days\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"VISUALIZATION: GOOG vs Other FAANG Stocks on Low Volume Days\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Get the 7 dates with GOOG's lowest volumes\n",
    "low_volume_dates = lowest_volume['date'].unique()\n",
    "\n",
    "# Convert to pandas Timestamps if they're numpy datetime64\n",
    "if isinstance(low_volume_dates[0], np.datetime64):\n",
    "    low_volume_dates = pd.to_datetime(low_volume_dates)\n",
    "\n",
    "# Sort dates\n",
    "low_volume_dates = sorted(low_volume_dates)\n",
    "\n",
    "# Filter original faang data for these dates\n",
    "dates_data = faang[faang['date'].isin(low_volume_dates)].copy()\n",
    "\n",
    "# Calculate volume for each ticker on these dates\n",
    "ticker_volumes = dates_data.groupby(['date', 'ticker'])[\n",
    "    'volume'].mean().unstack()\n",
    "\n",
    "# For comparison, get overall average volume for each ticker\n",
    "overall_avg = faang.groupby('ticker')['volume'].mean()\n",
    "\n",
    "# Create figure\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "# Plot data\n",
    "date_labels = []\n",
    "for d in low_volume_dates:\n",
    "    if hasattr(d, 'strftime'):\n",
    "        date_labels.append(d.strftime('%b %d, %Y'))\n",
    "    else:\n",
    "        # Convert to pandas Timestamp first\n",
    "        d_pd = pd.Timestamp(d)\n",
    "        date_labels.append(d_pd.strftime('%b %d, %Y'))\n",
    "\n",
    "x = np.arange(len(date_labels))\n",
    "width = 0.15\n",
    "\n",
    "# Plot bars for each ticker\n",
    "tickers_to_plot = ['GOOG', 'AAPL', 'AMZN', 'FB', 'NFLX']\n",
    "colors = {\n",
    "    'GOOG': 'red',\n",
    "    'AAPL': 'blue',\n",
    "    'AMZN': 'orange',\n",
    "    'FB': 'green',\n",
    "    'NFLX': 'purple'}\n",
    "\n",
    "for i, ticker in enumerate(tickers_to_plot):\n",
    "    # Get volumes for this ticker on these dates\n",
    "    ticker_vols = []\n",
    "    for date in low_volume_dates:\n",
    "        # Ensure date is in the index\n",
    "        if date in ticker_volumes.index:\n",
    "            if ticker in ticker_volumes.columns:\n",
    "                ticker_vols.append(ticker_volumes.loc[date, ticker])\n",
    "            else:\n",
    "                ticker_vols.append(0)\n",
    "        else:\n",
    "            ticker_vols.append(0)\n",
    "\n",
    "    # Calculate offset for this ticker's bars\n",
    "    offset = (i - len(tickers_to_plot)/2) * width + width/2\n",
    "\n",
    "    # Plot bars\n",
    "    bars = axes[0].bar(x + offset, ticker_vols, width,\n",
    "                       color=colors[ticker], edgecolor='black', linewidth=1,\n",
    "                       label=ticker, alpha=0.8)\n",
    "\n",
    "    # Add value labels for GOOG (the focus)\n",
    "    if ticker == 'GOOG':\n",
    "        for j, (bar, vol) in enumerate(zip(bars, ticker_vols)):\n",
    "            if vol > 0:\n",
    "                axes[0].text(bar.get_x() + bar.get_width()/2,\n",
    "                             bar.get_height(),\n",
    "                             f'{vol:,.0f}',\n",
    "                             ha='center',\n",
    "                             va='bottom',\n",
    "                             fontsize=12,\n",
    "                             fontweight='bold')\n",
    "\n",
    "axes[0].set_title(\n",
    "    'Volume Comparison: All FAANG Stocks on GOOG\\'s Lowest Volume Days',\n",
    "    fontsize=14,\n",
    "    fontweight='bold',\n",
    "    pad=20)\n",
    "axes[0].set_xlabel('Date', fontsize=12)\n",
    "axes[0].set_ylabel('Trading Volume (Shares)', fontsize=12)\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(date_labels, rotation=45, ha='right')\n",
    "axes[0].legend(title='Ticker', fontsize=10, title_fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3, axis='y', linestyle='--')\n",
    "\n",
    "# Add a note about the pattern\n",
    "axes[0].text(\n",
    "    0.02,\n",
    "    0.98,\n",
    "    'Pattern: GOOG consistently has\\nlowest volume on these days',\n",
    "    transform=axes[0].transAxes,\n",
    "    fontsize=10,\n",
    "    verticalalignment='top',\n",
    "    bbox=dict(\n",
    "        boxstyle='round',\n",
    "        facecolor='lightyellow',\n",
    "        alpha=0.8))\n",
    "\n",
    "# Calculate for each date: GOOG volume and average of other 4 stocks\n",
    "goog_volumes = []\n",
    "other_avg_volumes = []\n",
    "ratios = []\n",
    "\n",
    "for date in low_volume_dates:\n",
    "    if date in ticker_volumes.index:\n",
    "        goog_vol = ticker_volumes.loc[date, 'GOOG']\n",
    "        # Get average of other stocks (excluding GOOG)\n",
    "        other_stocks = [t for t in tickers_to_plot if t != 'GOOG']\n",
    "        other_vols = [ticker_volumes.loc[date, t]\n",
    "                      for t in other_stocks if t in ticker_volumes.columns]\n",
    "        other_avg = np.mean(other_vols) if other_vols else 0\n",
    "\n",
    "        goog_volumes.append(goog_vol)\n",
    "        other_avg_volumes.append(other_avg)\n",
    "        ratios.append(goog_vol / other_avg if other_avg > 0 else 0)\n",
    "    else:\n",
    "        goog_volumes.append(0)\n",
    "        other_avg_volumes.append(0)\n",
    "        ratios.append(0)\n",
    "\n",
    "# Plot GOOG volume vs Other average\n",
    "x_pos = np.arange(len(date_labels))\n",
    "bar_width = 0.35\n",
    "\n",
    "bars_goog = axes[1].bar(\n",
    "    x_pos - bar_width/2,\n",
    "    goog_volumes,\n",
    "    bar_width,\n",
    "    color='red',\n",
    "    alpha=0.7,\n",
    "    label='GOOG Volume',\n",
    "    edgecolor='darkred')\n",
    "bars_other = axes[1].bar(\n",
    "    x_pos + bar_width/2,\n",
    "    other_avg_volumes,\n",
    "    bar_width,\n",
    "    color='blue',\n",
    "    alpha=0.7,\n",
    "    label='Other FAANG Average',\n",
    "    edgecolor='darkblue')\n",
    "\n",
    "# Add value labels on bars with ratio\n",
    "for i, (bar_goog, bar_other, goog_vol, other_avg, ratio) in enumerate(\n",
    "        zip(bars_goog, bars_other, goog_volumes, other_avg_volumes, ratios)):\n",
    "    if goog_vol > 0:\n",
    "        # GOOG bar: Volume + Ratio\n",
    "        axes[1].text(bar_goog.get_x() + bar_goog.get_width()/2,\n",
    "                     bar_goog.get_height(),\n",
    "                     f'{goog_vol:,.0f}\\n({ratio:.2f}x)',\n",
    "                     ha='center',\n",
    "                     va='bottom',\n",
    "                     fontsize=12,\n",
    "                     fontweight='bold',\n",
    "                     bbox=dict(boxstyle='round',\n",
    "                               facecolor='white',\n",
    "                               alpha=0.8,\n",
    "                               edgecolor='red'))\n",
    "\n",
    "    if other_avg > 0:\n",
    "        # Other bar: Just volume\n",
    "        axes[1].text(bar_other.get_x() + bar_other.get_width()/2,\n",
    "                     bar_other.get_height(),\n",
    "                     f'{other_avg:,.0f}',\n",
    "                     ha='center',\n",
    "                     va='bottom',\n",
    "                     fontsize=12)\n",
    "\n",
    "axes[1].set_title(\n",
    "    'GOOG Volume vs Other FAANG Stocks Average',\n",
    "    fontsize=14,\n",
    "    fontweight='bold',\n",
    "    pad=20)\n",
    "axes[1].set_xlabel('Date', fontsize=12)\n",
    "axes[1].set_ylabel('Volume (Shares)', fontsize=12)\n",
    "axes[1].set_xticks(x_pos)\n",
    "axes[1].set_xticklabels(date_labels, rotation=45, ha='right')\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3, axis='y', linestyle='--')\n",
    "\n",
    "# Add summary statistics\n",
    "valid_ratios = [r for r in ratios if not np.isinf(r) and not np.isnan(r)]\n",
    "if valid_ratios:\n",
    "    avg_ratio = np.mean(valid_ratios)\n",
    "    summary_text = f\"Key Insight: On GOOG's lowest volume days:\\n\"\n",
    "    summary_text += f\"• GOOG volume is {avg_ratio:.2f}x other FAANG stocks\\n\"\n",
    "summary_text += (\n",
    "    f\"• GOOG is below others on {sum(r < 1 for r in valid_ratios)} \"\n",
    "    f\"of {len(valid_ratios)} days\\n\"\n",
    ")\n",
    "\n",
    "if min(valid_ratios) > 0:\n",
    "    summary_text += f\"• Maximum difference: {min(valid_ratios):.3f}x\"\n",
    "\n",
    "axes[1].text(0.02, 0.98, summary_text, transform=axes[1].transAxes,\n",
    "             fontsize=10, verticalalignment='center',\n",
    "             bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise 3 Summary:**\n",
    "\n",
    "From the full FAANG dataset (1,255 rows across five tickers), the seven lowest trading‑volume records all belong to Google (GOOG). These dates span May 24 to November 23, 2018, with volumes ranging from 679,000 to 887,400 shares. Compared to GOOG’s average daily volume of ~1.74 million shares, these lows represent only 39–51% of typical activity. \n",
    "\n",
    "Key Findings:\n",
    "- **Ticker concentration:** Only GOOG appears in the lowest‑volume subset, suggesting systematically lower trading activity relative to other FAANG stocks.  \n",
    "- **Date range:** Low‑volume days cluster within a 183‑day span in mid‑2018.  \n",
    "- **Volume context:** Even the lowest day (679,000 shares) still reflects significant market participation.  \n",
    "- **Data quality insight:** The exclusivity of GOOG in this subset may reflect genuine trading patterns, company‑specific events, or differences in how volumes are reported.\n",
    "\n",
    "Overall, this exercise highlights how outlier detection can reveal ticker‑specific trading characteristics and raises questions about whether observed anomalies stem from market behavior or data collection practices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:3px; background-color:black; border:none;\">\n",
    "\n",
    "## **Ch3 Ex4: Converting to Long Format**\n",
    "\n",
    "### **Objective:** \n",
    "\n",
    "Transform the dataframe from wide to completely long format.\n",
    "\n",
    "### **Steps to complete:**\n",
    "\n",
    "1. Identify 'date' and 'ticker' as ID variables\n",
    "2. Melt all other columns (open, high, low, close, volume) into long format\n",
    "3. Create two new columns: 'metric' and 'value'\n",
    "4. Sort the results\n",
    "5. Create faceted line plots for each metric\n",
    "6. Show summary statistics by metric\n",
    "\n",
    "<hr style=\"border-top:2px dashed blue;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# Step 1: Identify ID variables and value variables\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "id_vars = ['date', 'ticker']\n",
    "value_vars = ['open', 'high', 'low', 'close', 'volume']\n",
    "\n",
    "print(\"✓ Variables identified:\")\n",
    "print(f\"  ID variables (kept as columns): {id_vars}\")\n",
    "print(f\"  Value variables (to be melted): {value_vars}\")\n",
    "print(f\"  Original DataFrame shape: {faang.shape}\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Step 2: Melt dataframe to long format\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "faang_long = pd.melt(\n",
    "    faang,\n",
    "    id_vars=id_vars,\n",
    "    value_vars=value_vars,\n",
    "    var_name='metric',\n",
    "    value_name='value'\n",
    ")\n",
    "\n",
    "print(\"\\n✓ DataFrame melted to long format\")\n",
    "print(f\"  Long format shape: {faang_long.shape}\")\n",
    "print(f\"  Expansion: {faang_long.shape[0] / faang.shape[0]:.1f}x more rows\")\n",
    "\n",
    "print(\"\\nFirst 10 rows of long format data:\")\n",
    "display(faang_long.head(10))\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Step 3: Verify new columns and values\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "print(\"\\n✓ New columns created:\")\n",
    "print(f\"  'metric' column: {faang_long['metric'].nunique()} unique values\")\n",
    "print(f\"  'value' column data type: {faang_long['value'].dtype}\")\n",
    "\n",
    "print(\"\\nUnique metrics and their counts:\")\n",
    "metric_counts = faang_long['metric'].value_counts()\n",
    "for metric, count in metric_counts.items():\n",
    "    print(f\"  {metric}: {count:,} rows\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Step 4: Sort for better readability\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "faang_long = faang_long.sort_values(\n",
    "    ['date', 'ticker', 'metric']).reset_index(drop=True)\n",
    "\n",
    "print(\"\\n✓ Long format DataFrame sorted by date, ticker, and metric\")\n",
    "\n",
    "print(\"\\nSample of sorted data (AAPL on first date):\")\n",
    "sample_date = faang_long['date'].iloc[0]\n",
    "sample_ticker = 'AAPL'\n",
    "sample_data = faang_long[(faang_long['date'] == sample_date) & (\n",
    "    faang_long['ticker'] == sample_ticker)]\n",
    "display(sample_data)\n",
    "\n",
    "print(\"\\nStructure verification:\")\n",
    "print(f\"  Total rows: {len(faang_long):,}\")\n",
    "print(f\"  Unique dates: {faang_long['date'].nunique()}\")\n",
    "print(f\"  Unique tickers: {faang_long['ticker'].nunique()}\")\n",
    "print(f\"  Unique metrics: {faang_long['metric'].nunique()}\")\n",
    "expected_rows = faang_long['date'].nunique(\n",
    ") * faang_long['ticker'].nunique() * faang_long['metric'].nunique()\n",
    "print(f\"  Expected rows: {expected_rows:,}\")\n",
    "print(f\"  Actual rows match expected: {len(faang_long) == expected_rows}\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Step 5: Faceted line plots for each metric\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "g = sns.FacetGrid(faang_long, col='metric', col_wrap=3, height=4, sharey=False)\n",
    "g.map_dataframe(sns.lineplot, x='date', y='value', hue='ticker')\n",
    "g.add_legend()\n",
    "\n",
    "plt.subplots_adjust(top=0.9)\n",
    "g.fig.suptitle(\"FAANG Metrics Over Time (Long Format)\", fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Step 6: Summary statistics by metric\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "metric_summary = faang_long.groupby('metric')['value'].describe()\n",
    "print(\"\\nSummary statistics by metric:\")\n",
    "print(metric_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border-top:2px dashed blue;\">\n",
    "\n",
    "### **Exercise 4 Summary:**\n",
    "\n",
    "The FAANG dataset was successfully reshaped from wide to long format using `melt()`, expanding from 1,255 rows × 7 columns to 6,275 rows × 4 columns. Each original trading day now produces five records (open, high, low, close, volume), captured in a new `metric` column with corresponding `value`. \n",
    "\n",
    "This transformation not only standardizes the dataset but also enables richer analysis, such as faceted line plots by metric, metric‑level aggregations, and flexible visualizations across tickers and time. By moving to long format, the dataset is now optimized for comparative and time series analysis in subsequent exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:3px; background-color:black; border:none;\">\n",
    "\n",
    "## **Ch3 Ex5: Handling Data Glitch**\n",
    "\n",
    "### **Objective:** \n",
    "\n",
    "Determine the appropriate response to a data recording glitch.\n",
    "\n",
    "### **Given:** \n",
    "\n",
    "We have confirmed there was a glitch in how the data was recorded on July 26, 2018.\n",
    "\n",
    "<hr style=\"border-top:2px dashed blue;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Recommended Correction Approaches**\n",
    "\n",
    "### **Option 1: Source Data Correction (Most Preferred)**\n",
    "\n",
    "**Method:** \n",
    "\n",
    "Contact the original data provider(s) to obtain corrected records for July 26, 2018\n",
    "\n",
    "**Process:**\n",
    "\n",
    "1. Submit formal data correction request with specific error documentation\n",
    "2. Request reprocessing of affected date's data\n",
    "3. Obtain corrected data file or update from provider\n",
    "4. Validate corrected data against independent sources\n",
    "\n",
    "**Pros:**\n",
    "\n",
    "- Highest data accuracy\n",
    "- Maintains data provenance\n",
    "- Industry standard practice for financial data\n",
    "\n",
    "**Cons:**\n",
    "\n",
    "- Time-consuming (may take days/weeks)\n",
    "- Dependent on provider responsiveness\n",
    "- May incur additional costs\n",
    "\n",
    "\n",
    "\n",
    "### **Option 2: Statistical Imputation (When Source Correction Unavailable)**\n",
    "\n",
    "**Method:** \n",
    "\n",
    "Estimate correct values using statistical methods based on surrounding data\n",
    "\n",
    "**For Price Data (open, high, low, close):**\n",
    "\n",
    "- **Linear Interpolation:** Use July 25 and July 27 prices to estimate July 26 values\n",
    "- **Carry Forward:** Use July 25 closing prices for July 26 values\n",
    "- **Weighted Average:** Blend pre- and post-glitch period averages\n",
    "\n",
    "**For Volume Data:**\n",
    "\n",
    "- **Moving Average:** Use 20-day exponential moving average\n",
    "- **Same-Day Pattern:** Use volume patterns from previous weeks' same weekday\n",
    "- **Market-Adjusted:** Adjust based on overall market volume changes\n",
    "\n",
    "**Implementation Framework:**\n",
    "\n",
    "1. Create \"corrected_dataset\" with imputed values\n",
    "2. Maintain \"original_dataset\" with glitched data\n",
    "3. Add \"data_quality_flag\" column indicating imputed records\n",
    "4. Document imputation methodology in metadata\n",
    "\n",
    "\n",
    "### **Option 3: Data Exclusion with Annotation (Conservative Approach)**\n",
    "\n",
    "**Method:** \n",
    "\n",
    "Remove July 26, 2018 from analysis datasets with clear documentation\n",
    "\n",
    "**Implementation:**\n",
    "\n",
    "1. Create analysis dataset excluding July 26, 2018\n",
    "2. Add dataset documentation: \"July 26, 2018 excluded due to data recording glitch\"\n",
    "3. Use time series methods robust to missing data points\n",
    "4. For visualizations, show gap with explanatory note\n",
    "\n",
    "**When to Use:**\n",
    "\n",
    "- When data quality cannot be verified\n",
    "- For regulatory/compliance reporting\n",
    "- When imputation would introduce unacceptable uncertainty\n",
    "\n",
    "\n",
    "### **Option 4: Multi-Source Reconciliation (Comprehensive Approach)**\n",
    "\n",
    "**Method:** \n",
    "\n",
    "Cross-reference with multiple independent data sources\n",
    "\n",
    "**Process:**\n",
    "\n",
    "1. Collect July 26, 2018 data from 3+ alternative providers\n",
    "2. Identify consensus values where sources agree\n",
    "3. For discrepancies, use weighted average based on source reliability scores\n",
    "4. Create confidence intervals for each imputed value\n",
    "\n",
    "**Data Sources to Consider:**\n",
    "\n",
    "- Alternative financial data providers (Bloomberg, Reuters, Yahoo Finance)\n",
    "- Exchange-reported data\n",
    "- News sources reporting intraday prices\n",
    "- Social media sentiment analysis for volume context\n",
    "\n",
    "\n",
    "## **Recommended Implementation Plan**\n",
    "\n",
    "**Phase 1: Immediate Response**\n",
    "\n",
    "1. Quarantine affected data in separate \"under_investigation\" table\n",
    "2. Notify all data consumers of the issue\n",
    "3. Begin source correction requests\n",
    "\n",
    "**Phase 2: Short-term Solution (Days 1-7)**\n",
    "\n",
    "1. Implement Option 2 (Statistical Imputation) for urgent analyses\n",
    "2. Create \"best_estimate\" dataset with clear imputation flags\n",
    "3. Document all assumptions and methods\n",
    "\n",
    "**Phase 3: Long-term Solution (Week 2+)**\n",
    "\n",
    "1. Pursue Option 1 (Source Correction) as primary resolution\n",
    "2. If unsuccessful, implement Option 4 (Multi-Source Reconciliation)\n",
    "3. Update data governance policies to prevent recurrence\n",
    "\n",
    "**Phase 4: Prevention**\n",
    "\n",
    "1. Implement automated anomaly detection\n",
    "2. Establish data quality SLAs with providers\n",
    "3. Create data validation pipeline with multiple checkpoints\n",
    "\n",
    "### Correction Options Summary\n",
    "\n",
    "| Option | Method | Pros | Cons | Best Use Case |\n",
    "|--------|--------|------|------|---------------|\n",
    "| **1. Source Correction** | Request corrected records from provider | Highest accuracy, maintains provenance, industry standard | Slow, provider dependent, may incur costs | Long‑term resolution, compliance reporting |\n",
    "| **2. Statistical Imputation** | Estimate values using interpolation, averages, or moving windows | Fast, usable for urgent analysis, flexible | Less accurate, introduces assumptions | Short‑term fixes when source correction unavailable |\n",
    "| **3. Exclusion with Annotation** | Remove July 26, 2018 from dataset | Conservative, avoids bias, transparent | Data loss, gap in time series | Regulatory reporting, when imputation is too uncertain |\n",
    "| **4. Multi‑Source Reconciliation** | Cross‑check multiple providers and reconcile | Balanced, robust, adds confidence intervals | Complex, resource‑intensive | When correction unavailable and accuracy is critical |\n",
    "\n",
    "\n",
    "## **Summary: Root Cause Analysis & Alternative Explanations**\n",
    "\n",
    "**Important Note:** \n",
    "\n",
    "The following analysis considers what might explain the July 26, 2018 anomalies **IF** we hadn't confirmed a data recording glitch:\n",
    "\n",
    "### **Potential Non-Glitch Explanations for Observed Anomalies:**\n",
    "\n",
    "#### **1. Market Events (Most Likely)**\n",
    "\n",
    "**Facebook Earnings Report:**\n",
    "\n",
    "- Facebook reported Q2 2018 earnings after market close on July 25, 2018\n",
    "- Missed revenue expectations led to 19% stock drop on July 26\n",
    "- Explains FB's extreme volume (169.8M shares vs. 23.2M average)\n",
    "- Contagion effect likely affected other FAANG stocks\n",
    "\n",
    "**Supporting Evidence:**\n",
    "\n",
    "- Historical news confirms Facebook earnings date\n",
    "- 632% volume increase aligns with earnings surprise magnitude\n",
    "- Price decline pattern matches earnings disappointment reactions\n",
    "\n",
    "#### **2. Technical Factors**\n",
    "\n",
    "**Options Expiration:**\n",
    "\n",
    "- Monthly options expiration can cause unusual volume\n",
    "- July 27, 2018 was monthly options expiration (affecting July 26 trading)\n",
    "- \"Gamma squeeze\" effects can exaggerate price movements\n",
    "\n",
    "**Algorithmic Trading:**\n",
    "\n",
    "- High-frequency trading reactions to Facebook news\n",
    "- Momentum algorithms amplifying market moves\n",
    "- Stop-loss orders triggering cascading effects\n",
    "\n",
    "#### **3. Fundamental Factors**\n",
    "\n",
    "**Sector Rotation:**\n",
    "\n",
    "- Technology sector rebalancing\n",
    "- Institutional portfolio adjustments post-earnings\n",
    "- Hedge fund positioning changes\n",
    "\n",
    "**Macroeconomic Context:**\n",
    "\n",
    "- Trade war concerns in July 2018\n",
    "- Federal Reserve policy uncertainty\n",
    "- Overall market volatility period\n",
    "\n",
    "#### **4. Data Collection Artifacts**\n",
    "\n",
    "**Legitimate Data Variations:**\n",
    "\n",
    "- After-hours trading included in volume counts\n",
    "- International market trading affecting U.S. reported volumes\n",
    "- Data provider methodology differences\n",
    "\n",
    "### Final Recommendation\n",
    "\n",
    "For the July 26, 2018 data glitch, the most appropriate course of action is:\n",
    "\n",
    "- **Primary Resolution (Option 1: Source Correction):** Pursue corrected records directly from the original data provider to ensure accuracy and maintain provenance.  \n",
    "- **Interim Solution (Option 2: Statistical Imputation):** Apply imputation methods (e.g., interpolation, moving averages) for urgent analyses while awaiting corrected data.  \n",
    "- **Governance Measures:** Quarantine affected records, flag imputed values with a `data_quality_flag`, and document all assumptions.  \n",
    "- **Long-Term Safeguards:** If source correction is not possible, implement multi‑source reconciliation (Option 4) and strengthen anomaly detection pipelines to prevent recurrence.\n",
    "\n",
    "**Conclusion:**  \n",
    "This dual approach balances immediate analytical needs with long‑term data integrity. By combining imputation for short‑term usability with source correction for definitive accuracy, the dataset remains reliable for both exploratory analysis and compliance‑driven reporting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:3px; background-color:black; border:none;\">\n",
    "\n",
    "## **Ch3 Ex6: COVID-19 Data Cleaning and Pivoting**\n",
    "\n",
    "### **Objective:** \n",
    "\n",
    "Clean and pivot COVID-19 case data into wide format for time series analysis of selected countries.\n",
    "\n",
    "### **Specific Requirements:**\n",
    "\n",
    "- Read in the `covid19_cases.csv` file\n",
    "- Create a date column using the data in the `dateRep` column and the `pd.to_datetime()` function\n",
    "- Set the date column as the index and sort the index\n",
    "- Replace all occurrences of `United_States_of_America` and `United_Kingdom` with `USA` and `UK`, respectively\n",
    "- Using the `countriesAndTerritories` column, filter the cleaned COVID-19 cases data down to: Argentina, Brazil, China, Colombia, India, Italy, Mexico, Peru, Russia, Spain, Turkey, the UK, and the USA\n",
    "- Pivot the data so that the index contains the dates, the columns contain the country names, and the values are the case counts (the `cases` column)\n",
    "- Fill in NaN values with 0\n",
    "\n",
    "### **Steps to Complete:**\n",
    "\n",
    "1. Read in the `covid19_cases.csv` file\n",
    "2. Create a 'date' column using the 'dateRep' column\n",
    "3. Set 'date' as index and sort chronologically\n",
    "4. Standardize country names (USA and UK)\n",
    "5. Filter to 13 specific countries\n",
    "6. Pivot to wide format with countries as columns\n",
    "7. Fill NaN values with 0\n",
    "\n",
    "<hr style=\"border-top:2px dashed blue;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# Step 1: Read in the covid19_cases.csv file\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"STEP 1: Reading COVID-19 data file\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "covid_path = os.path.join(base_path, 'covid19_cases.csv')\n",
    "covid = pd.read_csv(covid_path)\n",
    "\n",
    "print(\"✓ COVID-19 data loaded successfully\")\n",
    "print(f\"  File: {covid_path}\")\n",
    "print(f\"  Original DataFrame shape: {covid.shape}\")\n",
    "print(f\"  Total records: {len(covid):,}\")\n",
    "print(f\"  Total columns: {len(covid.columns)}\")\n",
    "\n",
    "print(\"\\nFirst 5 rows of raw data:\")\n",
    "display(covid.head())\n",
    "\n",
    "print(\"\\nColumn names in dataset:\")\n",
    "for i, col in enumerate(covid.columns, 1):\n",
    "    print(f\"  {i:2d}. {col}\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Step 2: Create a 'date' column using the 'dateRep' column\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"STEP 2: Creating proper date column from dateRep\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "print(\"Before conversion - dateRep column:\")\n",
    "print(f\"  Data type: {covid['dateRep'].dtype}\")\n",
    "print(\"  Sample values:\")\n",
    "print(covid['dateRep'].head(3).to_string(index=False))\n",
    "\n",
    "# Convert dateRep to proper datetime\n",
    "covid['date'] = pd.to_datetime(covid['dateRep'], format='%d/%m/%Y')\n",
    "\n",
    "print(\"\\nAfter conversion - new date column:\")\n",
    "print(f\"  Data type: {covid['date'].dtype}\")\n",
    "print(\"  Sample values:\")\n",
    "print(covid['date'].head(3).to_string(index=False))\n",
    "print(f\"\\n  Date range: {covid['date'].min()} to {covid['date'].max()}\")\n",
    "print(\n",
    "    f\"  Total days in dataset: {(covid['date'].max() - covid['date'].min()).days + 1}\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Step 3: Set 'date' as index and sort chronologically\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"STEP 3: Setting date as index and sorting\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "print(\"Before setting index:\")\n",
    "print(f\"  Current index: {covid.index.name}\")\n",
    "print(f\"  Index type: {type(covid.index)}\")\n",
    "\n",
    "# Set date as index and sort\n",
    "covid.set_index('date', inplace=True)\n",
    "covid.sort_index(inplace=True)\n",
    "\n",
    "print(\"\\nAfter setting index:\")\n",
    "print(f\"  New index: {covid.index.name}\")\n",
    "print(f\"  Index type: {type(covid.index)}\")\n",
    "print(f\"  Is sorted: {covid.index.is_monotonic_increasing}\")\n",
    "\n",
    "print(\"\\nFirst 5 rows with date index:\")\n",
    "display(covid.head())\n",
    "print(\"\\nLast 5 rows with date index:\")\n",
    "display(covid.tail())\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Step 4: Standardize country names (USA and UK)\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"STEP 4: Standardizing country names\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "print(\"Before standardization:\")\n",
    "print(\"  Sample unique country names:\")\n",
    "print(\n",
    "    f\"    United_States_of_America: {(covid['countriesAndTerritories'] == 'United_States_of_America').sum():,} records\")\n",
    "print(\n",
    "    f\"    United_Kingdom: {(covid['countriesAndTerritories'] == 'United_Kingdom').sum():,} records\")\n",
    "print(\n",
    "    f\"    Total unique countries: {covid['countriesAndTerritories'].nunique()}\")\n",
    "\n",
    "# Replace country names\n",
    "covid['countriesAndTerritories'] = covid['countriesAndTerritories'].replace({\n",
    "    'United_States_of_America': 'USA',\n",
    "    'United_Kingdom': 'UK'\n",
    "})\n",
    "\n",
    "print(\"\\nAfter standardization:\")\n",
    "print(\"  Sample unique country names:\")\n",
    "print(\n",
    "    f\"    USA: {(covid['countriesAndTerritories'] == 'USA').sum():,} records\")\n",
    "print(f\"    UK: {(covid['countriesAndTerritories'] == 'UK').sum():,} records\")\n",
    "print(\n",
    "    f\"    Total unique countries: {covid['countriesAndTerritories'].nunique()}\")\n",
    "\n",
    "print(\"\\nSample of standardized data (showing USA and UK):\")\n",
    "usa_uk_sample = covid[covid['countriesAndTerritories'].isin(\n",
    "    ['USA', 'UK'])].head(5)\n",
    "display(usa_uk_sample)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Step 5: Filter to 13 specific countries\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"STEP 5: Filtering to 13 target countries\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Define target countries\n",
    "target_countries = [\n",
    "    'Argentina', 'Brazil', 'China', 'Colombia', 'India',\n",
    "    'Italy', 'Mexico', 'Peru', 'Russia', 'Spain',\n",
    "    'Turkey', 'UK', 'USA'\n",
    "]\n",
    "\n",
    "print(\"Target countries for analysis:\")\n",
    "for i in range(0, len(target_countries), 4):\n",
    "    print(f\"  {', '.join(target_countries[i:i+4])}\")\n",
    "\n",
    "print(f\"\\nTotal target countries: {len(target_countries)}\")\n",
    "\n",
    "# Filter data to target countries\n",
    "covid_filtered = covid[covid['countriesAndTerritories'].isin(\n",
    "    target_countries)].copy()\n",
    "\n",
    "print(f\"\\n✓ Data filtered successfully\")\n",
    "print(f\"  Original rows: {len(covid):,}\")\n",
    "print(f\"  Filtered rows: {len(covid_filtered):,}\")\n",
    "print(f\"  Percentage kept: {len(covid_filtered)/len(covid)*100:.1f}%\")\n",
    "\n",
    "print(\n",
    "    f\"\\nCountries in filtered data ({covid_filtered['countriesAndTerritories'].nunique()} total): \")\n",
    "countries_in_data = sorted(covid_filtered['countriesAndTerritories'].unique())\n",
    "for i in range(0, len(countries_in_data), 4):\n",
    "    print(f\"  {', '.join(countries_in_data[i:i+4])}\")\n",
    "\n",
    "print(\"\\nSample of filtered data (first 10 rows):\")\n",
    "display(covid_filtered.head(10))\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Step 6: Pivot to wide format with countries as columns\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"STEP 6: Pivoting to wide format\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "print(\"Before pivoting:\")\n",
    "print(f\"  Current shape: {covid_filtered.shape}\")\n",
    "print(f\"  Current format: Long format (multiple rows per date)\")\n",
    "\n",
    "# Pivot the data\n",
    "covid_wide = covid_filtered.pivot_table(\n",
    "    index=covid_filtered.index,           # Dates as rows\n",
    "    columns='countriesAndTerritories',    # Countries as columns\n",
    "    values='cases',                       # Case counts as values\n",
    "    aggfunc='sum'                         # Sum cases for each date-country combination\n",
    ")\n",
    "\n",
    "print(\"\\nAfter pivoting:\")\n",
    "print(f\"  New shape: {covid_wide.shape}\")\n",
    "print(f\"  Rows (dates): {len(covid_wide)}\")\n",
    "print(f\"  Columns (countries): {len(covid_wide.columns)}\")\n",
    "print(f\"  New format: Wide format (one row per date, one column per country)\")\n",
    "\n",
    "print(\"\\nFirst 10 rows of wide format data:\")\n",
    "display(covid_wide.head(10))\n",
    "\n",
    "print(\"\\nColumn names (countries) in wide format:\")\n",
    "for i in range(0, len(covid_wide.columns), 4):\n",
    "    print(f\"  {', '.join(covid_wide.columns[i:i+4])}\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Step 7: Fill NaN values with 0\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"STEP 7: Filling NaN values with 0\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "print(\"Before filling NaN values:\")\n",
    "print(f\"  Total NaN values: {covid_wide.isna().sum().sum():,}\")\n",
    "print(\n",
    "    f\"  Percentage NaN: {covid_wide.isna().sum().sum() / (covid_wide.shape[0] * covid_wide.shape[1]) * 100:.2f}%\")\n",
    "print(f\"  NaN values by column (first 5 columns):\")\n",
    "for col in covid_wide.columns[:5]:\n",
    "    nan_count = covid_wide[col].isna().sum()\n",
    "    print(\n",
    "        f\"    {col}: {nan_count:,} NaN values ({nan_count/len(covid_wide)*100:.1f}%)\")\n",
    "\n",
    "# Fill NaN with 0 and convert to integers\n",
    "covid_wide = covid_wide.fillna(0).astype(int)\n",
    "\n",
    "print(\"\\nAfter filling NaN values with 0:\")\n",
    "print(f\"  Total NaN values: {covid_wide.isna().sum().sum()}\")\n",
    "print(f\"  Data type of all columns: {covid_wide.dtypes.unique()[0]}\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Final Verification and Summary\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL VERIFICATION & SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n✓ ALL STEPS COMPLETED SUCCESSFULLY\")\n",
    "\n",
    "print(\"\\nFinal DataFrame Specifications:\")\n",
    "print(f\"  Shape: {covid_wide.shape}\")\n",
    "print(f\"  Index: {covid_wide.index.name} ({len(covid_wide)} dates)\")\n",
    "print(f\"  Columns: {len(covid_wide.columns)} countries\")\n",
    "print(f\"  Date range: {covid_wide.index.min()} to {covid_wide.index.max()}\")\n",
    "print(f\"  Data type: All values are integers\")\n",
    "print(f\"  Missing values: {covid_wide.isna().sum().sum()} (all filled with 0)\")\n",
    "\n",
    "print(\"\\nFirst 5 rows of final cleaned data:\")\n",
    "display(covid_wide.head())\n",
    "\n",
    "print(\"\\nLast 5 rows of final cleaned data:\")\n",
    "display(covid_wide.tail())\n",
    "\n",
    "print(\"\\nData Statistics:\")\n",
    "print(f\"  Total cases in dataset: {covid_wide.sum().sum():,}\")\n",
    "print(f\"  Average daily cases per country: {covid_wide.mean().mean():.1f}\")\n",
    "print(f\"  Maximum daily cases (single country): {covid_wide.max().max():,}\")\n",
    "\n",
    "print(\"\\nTop 5 countries by total cases:\")\n",
    "total_cases_by_country = covid_wide.sum().sort_values(ascending=False)\n",
    "for i, (country, cases) in enumerate(\n",
    "        total_cases_by_country.head(5).items(), 1):\n",
    "    print(f\"  {i}. {country}: {cases:,} cases\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Save the cleaned data\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"Saving cleaned data\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Save to CSV\n",
    "cleaned_file = os.path.join(base_path, 'covid19_cleaned_wide.csv')\n",
    "covid_wide.to_csv(cleaned_file)\n",
    "\n",
    "print(f\"✓ Cleaned data saved to: {cleaned_file}\")\n",
    "print(f\"  File size: {os.path.getsize(cleaned_file):,} bytes\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Visualization: COVID-19 Case Trends\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"VISUALIZATION: COVID-19 Case Trends\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create figure with 2x2 subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Plot 1: Total cases by country (bar chart)\n",
    "total_cases = covid_wide.sum().sort_values(ascending=False)\n",
    "top_10_total = total_cases.head(10)\n",
    "\n",
    "colors = plt.cm.Set3(range(len(top_10_total)))\n",
    "bars = axes[0, 0].bar(range(len(top_10_total)),\n",
    "                      top_10_total.values, color=colors)\n",
    "axes[0, 0].set_title('Top 10 Countries: Total COVID-19 Cases',\n",
    "                     fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Country', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Total Cases', fontsize=12)\n",
    "axes[0, 0].set_xticks(range(len(top_10_total)))\n",
    "axes[0, 0].set_xticklabels(top_10_total.index, rotation=45, ha='right')\n",
    "axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (bar, value) in enumerate(zip(bars, top_10_total.values)):\n",
    "    axes[0, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height(),\n",
    "                    f'{value:,.0f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# Plot 2: 7-day moving average for top 3 countries\n",
    "top_3_countries = total_cases.head(3).index.tolist()\n",
    "colors_ts = ['red', 'blue', 'green']\n",
    "\n",
    "for idx, country in enumerate(top_3_countries):\n",
    "    moving_avg = covid_wide[country].rolling(window=7).mean()\n",
    "    axes[0, 1].plot(moving_avg.index, moving_avg.values, label=country,\n",
    "                    color=colors_ts[idx], linewidth=2)\n",
    "\n",
    "axes[0, 1].set_title('7-Day Moving Average (Top 3 Countries)',\n",
    "                     fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Date', fontsize=12)\n",
    "axes[0, 1].set_ylabel('Daily Cases (7-day avg)', fontsize=12)\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Plot 3: Cumulative cases over time\n",
    "cumulative_cases = covid_wide.cumsum()\n",
    "for idx, country in enumerate(top_3_countries):\n",
    "    axes[1, 0].plot(cumulative_cases.index, cumulative_cases[country],\n",
    "                    label=country, color=colors_ts[idx], linewidth=2)\n",
    "\n",
    "axes[1, 0].set_title('Cumulative COVID-19 Cases (Top 3 Countries)',\n",
    "                     fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Date', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Cumulative Cases', fontsize=12)\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "axes[1, 0].ticklabel_format(axis='y', style='scientific', scilimits=(0, 0))\n",
    "\n",
    "# Plot 4: Heatmap of cases by month\n",
    "covid_wide['month'] = covid_wide.index.strftime('%Y-%m')\n",
    "monthly_cases = covid_wide.groupby('month')[top_3_countries].sum()\n",
    "\n",
    "im = axes[1, 1].imshow(monthly_cases.T.values, aspect='auto', cmap='YlOrRd')\n",
    "axes[1, 1].set_title('Monthly Cases Heatmap (Top 3 Countries)',\n",
    "                     fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Month', fontsize=12)\n",
    "axes[1, 1].set_ylabel('Country', fontsize=12)\n",
    "axes[1, 1].set_xticks(range(len(monthly_cases.index)))\n",
    "axes[1, 1].set_xticklabels([m[5:] for m in monthly_cases.index], rotation=45)\n",
    "axes[1, 1].set_yticks(range(len(top_3_countries)))\n",
    "axes[1, 1].set_yticklabels(top_3_countries)\n",
    "plt.colorbar(im, ax=axes[1, 1], label='Monthly Cases')\n",
    "\n",
    "plt.suptitle('COVID-19 Data Analysis: January - September 2020',\n",
    "             fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border-top:2px dashed blue;\">\n",
    "\n",
    "### **Exercise 6 Summary:**\n",
    "\n",
    "Successfully cleaned and transformed COVID-19 case data into wide format for 13 target countries. \n",
    "The dataset now features dates as rows and countries as columns, with daily case counts as values. \n",
    "All missing values were filled with 0, country names standardized (USA, UK), and the data chronologically sorted. \n",
    "This structured format enables efficient time series analysis, country comparisons, and visualization of pandemic progression from January to September 2020.\n",
    "\n",
    "| Rank | Country | Total Cases |\n",
    "|------|---------|-------------|\n",
    "| 1    | USA     | 6,674,458   |\n",
    "| 2    | India   | 5,214,677   |\n",
    "| 3    | Brazil  | 4,455,386   |\n",
    "| 4    | Russia  | 1,085,281   |\n",
    "| 5    | Peru    | 750,098     |\n",
    "\n",
    "With the dataset in wide format, we produced comparative visualizations (bar chart, moving averages, cumulative trends, heatmap) to highlight pandemic progression across countries. This exercise underscores the importance of rigorous preprocessing for reliable epidemiological analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:3px; background-color:black; border:none;\">\n",
    "\n",
    "## Week 1 & 2 Coding Assignment Reflection\n",
    "\n",
    "### Overall Assessment\n",
    "\n",
    "This assignment provided a comprehensive walkthrough of data cleaning, reshaping, and analysis using Pandas. By working with both financial (FAANG) and epidemiological (COVID-19) datasets, I gained practical experience in handling real-world data challenges such as merging files, type conversions, reshaping formats, anomaly detection, and visualization. The step-by-step structure reinforced the importance of validation at each stage and highlighted how preprocessing directly impacts the quality of downstream analysis.\n",
    "\n",
    "### Conceptually Easy Portions\n",
    "\n",
    "- **File Combination (Exercise 1):** Merging multiple CSVs and adding a `ticker` column was straightforward and intuitive.  \n",
    "- **Type Conversion and Sorting (Exercise 2):** Converting columns to appropriate datatypes and ordering the dataset felt natural and aligned with prior experience.  \n",
    "- **Basic Filtering (Exercise 6):** Selecting target countries and pivoting to wide format was conceptually clear once the requirements were defined.\n",
    "\n",
    "### More Difficult Portions\n",
    "\n",
    "- **Long-to-Wide Reshaping (Exercise 4):** Understanding the melt/pivot process required careful attention to ID vs. value variables and verifying structure.  \n",
    "- **Data Glitch Handling (Exercise 5):** Developing multiple correction strategies and considering governance implications was more conceptual and required critical thinking beyond coding.  \n",
    "- **Visualization (Exercises 3 & 6):** Designing meaningful plots and interpreting them in context demanded both technical skill and analytical reasoning.\n",
    "\n",
    "### Start / Stop / Continue\n",
    "\n",
    "- **Start:** Incorporating more narrative interpretation alongside code outputs, ensuring summaries explain *why* transformations matter.  \n",
    "- **Stop:** Relying solely on mechanical descriptions of steps; instead, emphasize insights and implications of the results.  \n",
    "- **Continue:** Validating data at each stage, documenting assumptions, and using structured summaries to communicate findings clearly.\n",
    "\n",
    "<hr style=\"height:4px; background-color:black; border:none;\">"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
