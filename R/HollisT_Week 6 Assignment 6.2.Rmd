---
title: "HollisT_Week6 Assignment 6.2"
author: "Tim Hollis"
date: "`r Sys.Date()`"
output: html_document
---



```{r global-setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)

# Load libraries
library(ggplot2)
library(tidyr)
library(dplyr)
library(cowplot)

# Set ggplot2 theme globally
theme_set(theme_minimal())

# Set seed for reproducibility
set.seed(42)
```

# The Law of Large Numbers: Bigger Really is Better (When It Comes to Sample Sizes)

We will explore and demonstrate one of the most fundamental concepts in 
probability: the Law of Large Numbers. This principle is the "why" behind the 
headline's claim, it assures us that as a sample size grows, the experimental 
average of our results will get closer and closer to the theoretical expected 
value. We will bring this concept to life through simulations and examples, and
we will see precisely why **bigger really is better** in the world of statistics.


### **Die Roll Simulation**

We begin by simulating repeated rolls of a fair six-sided die. The expected 
value of a single roll is 3.5, but this value is not actually possible on any 
individual roll, the only possibilities are whole numbers 1,2,3,4,5, and 6, so 
the 3.5 is never itself going to be a value obtained when rolling the die, 
instead a cumulative average. We'll explore how the average of many rolls 
converges toward this expected value.

```{r die-roll-simulation}

# Simulate 100 rolls
rolls_100 <- sample(1:6, size = 100, replace = TRUE)
avg_100 <- cumsum(rolls_100) / seq_along(rolls_100)
df_100 <- data.frame(Roll = 1:100, Average = avg_100)

plot_100 <- ggplot(df_100, aes(x = Roll, y = Average)) +
  geom_line(color = '#FF6F61') +
  geom_hline(yintercept = 3.5, linetype = 'dashed', color = '#2E86C1') +
  labs(title = 'Average of First 100 Rolls', x = 'Roll Number', y = 'Running Average')

# Simulate 10,000 rolls
rolls_10000 <- sample(1:6, size = 10000, replace = TRUE)
avg_10000 <- cumsum(rolls_10000) / seq_along(rolls_10000)
df_10000 <- data.frame(Roll = 1:10000, Average = avg_10000)

plot_10000 <- ggplot(df_10000, aes(x = Roll, y = Average)) +
  geom_line(color = '#28B463') +
  geom_hline(yintercept = 3.5, linetype = 'dashed', color = '#AF7AC5') +
  labs(title = 'Average of First 10,000 Rolls', x = 'Roll Number', y = 'Running Average')

# Display side by side
plot_grid(plot_100, plot_10000, labels = c('A', 'B'), ncol = 2)
```

#### **Summary**

The plot shows that while individual rolls vary widely, the more you roll the 
die, the closer the average gets to the expected value of 3.5. Around 10,000 
rolls, the running average becomes nearly indistinguishable from 3.5. The 
running average stabilizes near this value as the number of rolls increases. 
This illustrates the Law of Large Numbers in action.

### **Red vs. Green Ball Draws**

Next, we simulate drawing balls from a box containing 30 red and 15 green balls,
with replacement. The expected proportion of red balls is 30/45 = 0.6667 and 
green is 15/45 = .3333. We'll visualize how the observed proportion converges 
over time

```{r ball-draw-simulation}

# Simulate 100 draws
set.seed(123)
colors <- c(rep('red', 30), rep('green', 15))
draws_100 <- sample(colors, size = 100, replace = TRUE)
red_100 <- cumsum(draws_100 == 'red') / seq_along(draws_100)
green_100 <- cumsum(draws_100 == 'green') / seq_along(draws_100)
df_100 <- data.frame(Draw = 1:100, Red = red_100, Green = green_100)
df_100_long <- pivot_longer(df_100, cols = c('Red', 'Green'), names_to = 'Color', values_to = 'Proportion')

# Simulate 10,000 draws
draws_10000 <- sample(colors, size = 10000, replace = TRUE)
red_10000 <- cumsum(draws_10000 == 'red') / seq_along(draws_10000)
green_10000 <- cumsum(draws_10000 == 'green') / seq_along(draws_10000)
df_10000 <- data.frame(Draw = 1:10000, Red = red_10000, Green = green_10000)
df_10000_long <- pivot_longer(df_10000, cols = c('Red', 'Green'), names_to = 'Color', values_to = 'Proportion')

# Plot for 100 draws
plot_100 <- ggplot(df_100_long, aes(x = Draw, y = Proportion, color = Color)) +
  geom_line(size = 1.2) +
  geom_hline(yintercept = 30/45, linetype = 'dashed', color = '#FF6F61') +
  geom_hline(yintercept = 15/45, linetype = 'dashed', color = '#28B463') +
  annotate('text', x = 50, y = 0.6667 + 0.05, label = 'Expected: 0.6667', fontface = 'bold', color = '#FF6F61', size = 4) +
  annotate('text', x = 50, y = 0.3333 + 0.05, label = 'Expected: 0.3333', fontface = 'bold', color = '#28B463', size = 4) +
  scale_color_manual(values = c('Red' = '#FF6F61', 'Green' = '#28B463')) +
  labs(title = 'Proportions After 100 Draws', x = 'Draw Number', y = 'Proportion')

# Plot for 10,000 draws
plot_10000 <- ggplot(df_10000_long, aes(x = Draw, y = Proportion, color = Color)) +
  geom_line(size = 1.2) +
  geom_hline(yintercept = 30/45, linetype = 'dashed', color = '#FF6F61') +
  geom_hline(yintercept = 15/45, linetype = 'dashed', color = '#28B463') +
  annotate('text', x = 5000, y = 0.6667 + 0.05, label = 'Expected: 0.6667', fontface = 'bold', color = '#FF6F61', size = 4) +
  annotate('text', x = 5000, y = 0.3333 + 0.05, label = 'Expected: 0.3333', fontface = 'bold', color = '#28B463', size = 4) +
  scale_color_manual(values = c('Red' = '#FF6F61', 'Green' = '#28B463')) +
  labs(title = 'Proportions After 10,000 Draws', x = 'Draw Number', y = 'Proportion')

# Display stacked
plot_grid(plot_100, plot_10000, labels = c('A', 'B'), nrow = 2)
```

#### **Summary**

In both the die roll and ball draw simulations, we observed how empirical 
averages and proportions converge toward their theoretical expectations as the 
number of trials increases. The die roll average stabilized near 3.5, despite 
no single roll ever producing that value, a clear illustration of the Law of 
Large Numbers. Similarly, the proportion of red and green balls drawn from the 
box approached 0.6667 and 0.3333 respectively, reinforcing the reliability of 
long-run frequencies in probabilistic systems.

These simulations highlight the importance of sample size in reducing 
variability and improving the accuracy of estimates. Early fluctuations are 
common, but with enough trials, randomness gives way to predictability.

### **LLN: Comparing SDs and Means**

To reinforce the concept of convergence, we now compare how both sample means 
and sample standard deviations behave as sample size increases. 

```{r LLN-sd-mean-comparison}

# Population with known standard deviation

#Programming: rnorm() generates a standard normal distribution. We then normalize it to SD = 1 and rescale to SD = 2.4.

#Statistics: This ensures our population has a known, fixed standard deviation — critical for testing convergence.


pop_std <- 2.4
populationN <- 10^6
population <- rnorm(populationN)
population <- population / sd(population) * pop_std

# Sample sizes

samplesizes <- seq(10, 1000)

# Initialize vectors

samplestds <- numeric(length(samplesizes))
samplemeans <- numeric(length(samplesizes))

# Run simulation

for (i in seq_along(samplesizes)) {
  sample <- sample(population, size = samplesizes[i], replace = TRUE)
  samplestds[i] <- sd(sample)
  samplemeans[i] <- mean(sample)
}

# Create data frames

df_sd <- data.frame(SampleSize = samplesizes, Value = samplestds, Type = 'Standard Deviation')
df_mean <- data.frame(SampleSize = samplesizes, Value = samplemeans, Type = 'Mean')
df_combined <- rbind(df_sd, df_mean)

#Plot

plot_sd <- ggplot(df_sd, aes(x = SampleSize, y = Value)) +
  geom_point(color = '#FF6F61', fill = '#FADBD8', shape = 22, size = 3, stroke = 0.8) +
  geom_hline(yintercept = pop_std, linetype = 'dashed', color = '#2E86C1', linewidth = 1.2) +
annotate('text', x = 500, y = pop_std + 0.25, label = 'Population SD = 2.4',
         fontface = 'bold', color = 'black', size = 5) +
  labs(title = 'Convergence of Sample Standard Deviations',
       x = 'Sample Size',
       y = 'Standard Deviation')

plot_mean <- ggplot(df_mean, aes(x = SampleSize, y = Value)) +
  geom_point(color = '#28B463', fill = '#D5F5E3', shape = 21, size = 3, stroke = 0.8) +
  geom_hline(yintercept = 0, linetype = 'dashed', color = '#AF7AC5', linewidth = 1.2) +
  annotate('text', x = 500, y = 0 + 0.25, label = 'Population Mean = 0',
         fontface = 'bold', color = 'black', size = 5) +
  labs(title = 'Convergence of Sample Means',
       x = 'Sample Size',
       y = 'Mean')

# Display vertically stacked
plot_grid(plot_sd, plot_mean, labels = c('A', 'B'), nrow = 2)
```


#### **Summary**

While the Law of Large Numbers guarantees convergence of sample means to the 
population mean (in this case, 0), we observe that sample standard deviations 
also stabilize around the true population value of 2.4.

Panel A shows the sample standard deviations converging toward 2.4, with early 
fluctuations that diminish as sample size grows. Panel B shows the sample means 
clustering around 0, with even tighter convergence due to the central limit theorem.

This comparison highlights that while LLN is formally defined for means, the 
principle of convergence applies broadly to other descriptive statistics, 
especially when samples are large and randomly drawn.


# Reflection on Assignment 1 of 2 this week

### **What Was Easy or Logical**

- Conceptual clarity: The Law of Large Numbers (LLN) was intuitive to demonstrate
through simulations. The idea that averages stabilize with larger samples felt 
logical, especially when visualized with cumulative plots.
- Code modularity: Structuring simulations into clear blocks (die rolls, ball 
draws, SD/mean comparisons) made the workflow easy to follow and debug.
- Visual storytelling: Using ggplot2 and cowplot to juxtapose small vs. large 
sample plots helped reinforce the narrative — bigger really is better.
- Annotation and color choices: Labeling expected values and using distinct color
palettes made the plots visually digestible and pedagogically effective.

### **What Was Difficult or Less Intuitive**

- Vector initialization and loop logic: Setting up the simulation for varying 
sample sizes required careful indexing and pre-allocation of vectors, which was 
less intuitive than single-size simulations.
- Standard deviation convergence: Unlike means, SDs fluctuated more and converged 
less tightly, which made interpretation trickier and required deeper statistical
reasoning.
- Plot stacking and layout: Getting plot_grid() to behave consistently across 
horizontal and vertical arrangements took some trial and error, especially with 
label placement and sizing.

### **What I Will Stop Doing**

- Hardcoding values without commentary: I’ll avoid inserting constants (like 2.4
or 0.6667) without explaining their origin or statistical relevance. Every number deserves 
a pedagogical anchor.
- Over-relying on default ggplot themes: While theme_minimal() is clean, I’ll
stop assuming it’s always the best fit — sometimes more contrast or gridlines 
help clarify convergence.

### **What I Will Continue Doing**

- Using cumulative averages and proportions: These are powerful tools for
demonstrating convergence and will remain a staple in my simulation toolkit.
- Annotating plots for clarity: Bold labels, dashed lines, and color-coded 
expectations make statistical concepts more accessible — I’ll keep refining this 
visual polish.
- Structuring code for readability: Chunking simulations, labeling plots, and
using consistent naming conventions helped both me and my instructor follow the 
logic — I’ll continue prioritizing clarity and maintainability.



